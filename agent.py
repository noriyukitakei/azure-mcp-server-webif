import asyncio
import os
import json
from fastmcp import Client
from openai import AzureOpenAI
from dotenv import load_dotenv
import streamlit as st

load_dotenv(verbose=True)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL")
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")
API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")
ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_SUBSCRIPTION_ID = os.getenv("AZURE_SUBSCRIPTION_ID")
MAX_STEPS = int(os.getenv("MAX_STEPS"))

# Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
client = AzureOpenAI(
    api_key=API_KEY,
    api_version=API_VERSION,
    azure_endpoint=ENDPOINT,
)

# MCP ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•è¨­å®š
config = {
    "mcpServers": {
        "local_server": {
            # Local stdio server
            "transport": "stdio",
            "command": "docker",
            "args": ["run", "-i", "--rm", "--env-file", ".env", "mcr.microsoft.com/azure-sdk/azure-mcp:latest"],
        }
    }
}
# MCP ã‚µãƒ¼ãƒãƒ¼ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œã™ã‚‹éåŒæœŸé–¢æ•°
async def run_agent(user_input):
    async with Client(config) as mcp:
        # MCP ã‚µãƒ¼ãƒãƒ¼ã®ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
        tools = await mcp.list_tools()  # FastMCPã‚µãƒ¼ãƒãƒ¼ã®ãƒ„ãƒ¼ãƒ«ä¸€è¦§
        print(f"ğŸ”§ MCP ã‚µãƒ¼ãƒãƒ¼ã§åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«: {[t.name for t in tools]}")
    
        # MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚¹ã‚­ãƒ¼ãƒã‚’ OpenAI Function Calling ç”¨ã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›
        functions = []
        for t in tools:
            functions.append({
                "name": t.name,
                "description": t.description,
                "parameters": t.inputSchema  # FastMCP ãŒ JSON Schema ã‚’æä¾›ã—ã¦ã„ã‚‹
            })
    
        # æœ€åˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ã¯ä¸€åº¦ã ã‘å…¥ã‚Œã‚‹
        context = []
        context.append({"role": "user", "content": user_input})
        for step in range(MAX_STEPS):
            print(f"\n=== æ¨è«–ã‚¹ãƒ†ãƒƒãƒ— {step + 1} ===")
    
            # ãƒ¦ãƒ¼ã‚¶å…¥åŠ›ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ãƒãƒ£ãƒƒãƒˆè£œå®Œã‚’å®Ÿè¡Œ
            response = client.chat.completions.create(
                model=DEPLOYMENT_NAME,  # Azure OpenAI ã§ã¯ model=ãƒ‡ãƒ—ãƒ­ã‚¤å
                # ã™ã§ã« context ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ã‚„å‰æ®µã®é–¢æ•°å®Ÿè¡ŒçµæœãŒå…¥ã£ã¦ã„ã‚‹ã®ã§ãã®ã¾ã¾æ¸¡ã™
                messages=context,
                functions=functions,
                function_call="auto"
            )
    
            msg = response.choices[0].message
    
            # å®Ÿè¡Œã™ã¹ãé–¢æ•°ãŒã‚ã‚‹ã‹ç¢ºèª
            if msg.function_call:
                func_name = msg.function_call.name
                args = json.loads(msg.function_call.arguments or "{}")
                print(f"é–¢æ•°å‘¼ã³å‡ºã—: {func_name}({args})")
    
                # MCPã‚µãƒ¼ãƒãƒ¼ã§é–¢æ•°ã‚’å®Ÿè¡Œ
                result = await mcp.call_tool(func_name, arguments=args)
                print(f"å®Ÿè¡Œçµæœ: {result}")
    
                # é–¢æ•°å®Ÿè¡Œçµæœã®å–å¾—(æ§‹é€ åŒ–ã•ã‚ŒãŸJSONãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆ)
                result_content = result.structured_content or result.content[0].text

                # MCPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®çµæœã‚’LLMã«å†å…¥åŠ›
                context.append(msg)
                context.append({
                    "role": "function",
                    "name": func_name,
                    "content": result_content
                })
            else:
                # æœ€çµ‚å›ç­”
                print("\nAI ã®æœ€çµ‚å›ç­”:")
                print(msg.content)
                return msg.content
    
    print("æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—ã«é”ã—ã¾ã—ãŸã€‚æœ€çµ‚å¿œç­”:")
    return msg.content


# ã“ã“ã‹ã‚‰ã¯ç”»é¢ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰
# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚
if "history" not in st.session_state:
    st.session_state["history"] = []

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
for message in st.session_state.history:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè³ªå•ã‚’å…¥åŠ›ã—ãŸã¨ãã®å‡¦ç†ã‚’è¨˜è¿°ã™ã‚‹ã€‚
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸè³ªå•ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    with st.chat_message("user"):
        st.write(prompt)

    # ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ ã™ã‚‹
    st.session_state.history.append({"role": "user", "content": prompt})

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«run_agenté–¢æ•°ã‚’å‘¼ã³å‡ºã™ã€‚
    response = asyncio.run(run_agent(prompt))

    # å›ç­”ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    with st.chat_message("assistant"):
        st.write(response)

    # å›ç­”ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ ã™ã‚‹ã€‚
    st.session_state.history.append({"role": "assistant", "content": response})